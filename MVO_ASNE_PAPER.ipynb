{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "This Python3 notebook is used to generate most of the figures for the paper I am writing for Seismological Research Letters to describe capture, recovery and conversion of data from the Montserrat Volcano Observatory's analog seismic network (ASN) that was installed by the Volcano Disaster Assistance Program (VDAP) in 1995. Then ASN ran until December 2004 although the VDAP system was heavily modified on March 2nd, 2001, when PC-SEIS/XDETECT was swapped for the QNX/Seislog PC and BRAINS (SUDSPick, Hypo71) became obsolete.\n",
    "\n",
    "<h1>Instructions:</h1>\n",
    "\n",
    "<h3>Creation of CSVDB:</h3>\n",
    "<ol>\n",
    "    <li> cd to the root directory of this repo</li>\n",
    "    <li>run BIN/seisanwavdb2csv.py. This will create any missing ASNE_wavefilesYYYYMM.csv files. These have one line for each trace in each wavfile.</li>\n",
    "    <li>run count_traces_per_day.py. This will create ASNE_dailytraceid_wavfiles_df.csv. This has one line for each trace, every day. This is used to create the station and site on-time plots.</li>\n",
    "</ol>\n",
    "There can be missing days, where there was no data. This is what we use the fill_missing_days_in_df function for.\n",
    "\n",
    "<h3>SUDS/Hypo71 to Seisan database conversion</h3>\n",
    "Conversion of SUDS files to Miniseed, registering them into Seisan database in Nordic format (S-files), and then coverting PHA and PUN into Nordic format\n",
    "\n",
    "<ol>\n",
    "    <li>To learn how to convert a single event, use the vdap2seisanDB Python notebook. Each event must have either a WVM or DMX file (waveform data) and may also have a PHA file (phase pick readings) and a PUN file (Hypo71 summary file)</li>\n",
    "    <li>To convert an entire collection of WVM, DMX, PHA and PUN files, use vdap2seisanDB.py.</li> \n",
    "    <li>A special version of this is montserrat_vdap2seisanDB.py, which has additional complexities in it unique to how the MVO data have been recovered and converted at different times.</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<h3>Background:</h3>\n",
    "\n",
    "ASN waveform data was converted 3 times:\n",
    "\n",
    "    2001: \n",
    "    \n",
    "    I had copies of IRIG.EXE, DEMUX.EXE from UEA Belham project I think?. and I modified sudsei.for from Seisan so that I could drive it from the command line. So I constructed Perl script to loop over all year/month directories of SUDS data, and time correct, demutiplex and convert to Seisan. I thought I even went through and associated all IRIG files into the MVOE database. I wonder where my original copy of the MVO REA/MVOE_ database went?\n",
    "        \n",
    "    2015:\n",
    "        \n",
    "    I used PC-SUDS or WIN-SUDS (which contained IRIG.EXE and DEMUX.EXE) and CONVSEIS (SUDS2GSE.EXE). So a Perl script on a 32-bit Windows PC was able to convert everything to GSE-1 format. And then I wrote ObsPy script to convert everything to Miniseed. Miniseed files were organized into an Antelope database, and then included when building an SDS archive of the combined networks.\n",
    "    \n",
    "    2019: \n",
    "    \n",
    "    For the student project, I copied the ASN Miniseed event waveform data into a Seisan DB called ASNE. I also tested if conversion still worked, and on a 64-bit windows 10 laptop was able to run PC-SUDS DEMUX.EXE and SUD2MSED.EXE to convert some SUDS WVM to SUDS DMX and MiniSEED files.\n",
    "    \n",
    "    However, this was unsatisfying. So I decided to reconvert everything. I think this is in suds2seisandb.py and a similarly named notebook. I converted DMX files into SAC files and then into Miniseed files. If this did not work, I tried to find the corresponding GSE file from 2015 conversion. DMX and GSE files do not have any time conversions added, and I have not included any in the SAC or Miniseed version, so the ASNE_ Miniseed database on my Windows laptop is in exactly the same time zone as originally recorded. Was this recorded in local time (UTC - 4) or in UTC?\n",
    "\n",
    "<i>Author: Glenn Thompson, 2019</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Issues discovered so far:</h3>\n",
    "\n",
    "* Sometimes with ASN WAV files we noticed that 1 file comes 1-s after another. Probably registered twice? Need to eliminate the duplicates.\n",
    "\n",
    "* The gaps in the WAV file ontime plot doesn't appear to correspond to the gaps found. Indeed, the gaps seem a lot less than show, e.g. early 2004.\n",
    "\n",
    "* I do not have a copy of the IRIG Seisan DB, which I made at MVO. It sounds like BGS have a copy, from reading Richard Luckett's report. I believe this was all the data from XDETECT that I originally converted in 2001, after it was phased out.\n",
    "\n",
    "* It appears that ASNE data (converted in 2015) are 4 hours behind SPN data (which we triggered in real-time by QNX, and appear to match DSN (MVO) data. This has only been seen in late March 2001 so far, which was after the transition. \n",
    "\n",
    "<b>Possibilities:</b>\n",
    "\n",
    "1. All ASNE WAV files have a timestamp that is too great by 4 hours. This seems possible if a 4 hour time correction was ever applied during my 2015 conversions. Were XDETECT files were timestamped with UTC or local time? Logbooks likely matched. So they might all have been UTC! If I have inadvertently added 4 hours to all ASNE WAV files, this means that new S-files that students are generating are off by 4 hours also. This is the worst case scenario. But it is also possible that XDETECT used localtime, in which case I did need to apply a 4 hour correction. Could I have applied the time correction twice?\n",
    "\n",
    "2. Only ASNE data after mid-March 2001 is off by 4 hours. I might have applied some blanket time correction to the data deriving from XDETECT (appropriate) and also from QNX Seislog (mistake). In this case I can probably throw away all ASN files after mid-March 2001 and use SPN files instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Read in libraries and define functions</u></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import obspy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import datetime as dt\n",
    "\n",
    "def get_yticks(thisdf):\n",
    "    yticklabels = []\n",
    "    yticks=[]\n",
    "    i = 0\n",
    "    for index, row in thisdf.iterrows():\n",
    "        date = str(index)\n",
    "        if date[-2:]=='01':\n",
    "            yticklabels.append(date)\n",
    "            yticks.append(i)\n",
    "        i+=1\n",
    "    ystep = 1\n",
    "    if len(yticks)>15:\n",
    "        ystep=2  \n",
    "    if len(yticks)>25:\n",
    "        ystep=3\n",
    "    if len(yticks)>40:\n",
    "        ystep=4\n",
    "    if len(yticks)>60:\n",
    "        ystep=6\n",
    "    if len(yticks)>120:\n",
    "        ystep=12    \n",
    "    yticks = yticks[0::ystep]\n",
    "    yticklabels = yticklabels[0::ystep]\n",
    "    return (yticks, yticklabels)\n",
    "\n",
    "def ontime_plot(data,xticklabels,yticks,yticklabels):\n",
    "    # make ontime plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax.imshow(data, aspect='auto', cmap=plt.cm.gray, interpolation='nearest')\n",
    "    ax.imshow(np.transpose(data), aspect='auto', cmap=plt.cm.gray, interpolation='nearest')\n",
    "    #plt.xticks(np.arange(len(xticklabels)), xticklabels)\n",
    "    plt.yticks(np.arange(len(xticklabels)), xticklabels)\n",
    "    #ax.set_xticklabels(xticklabels, rotation = 90)\n",
    "    ax.set_yticklabels(xticklabels, rotation = 0)\n",
    "    #plt.yticks(yticks, yticklabels)\n",
    "    plt.xticks(yticks, yticklabels, rotation = 90)\n",
    "    plt.tight_layout()\n",
    "    return (fig, ax)\n",
    "\n",
    "\n",
    "def numrunningperday_plot_deprecated(data, yticks, yticklabels):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    rect_main = [0.1, 0.1, 0.5, 0.75]\n",
    "    rect_side = [0.7, 0.1, 0.2, 0.75]\n",
    "\n",
    "    ax_main = plt.axes(rect_main)\n",
    "    #ax1 = fig.add_subplot(121)\n",
    "    s = np.sum(data,axis=1)\n",
    "    ax_main.plot(s,'.',color='black',markersize=1)\n",
    "    #plt.plot(s,'.',color='black',markersize=1)\n",
    "    #plt.xticks(yticks, yticklabels)\n",
    "    ax_main.set_xticks(yticks, yticklabels)\n",
    "    #ax1.set_xticklabels(yticklabels, rotation = 90)\n",
    "    ax_main.set_xticklabels(yticklabels, rotation = 90)\n",
    "    #plt.ylabel('number\\noperational')\n",
    "    ax_main.set_ylabel('number\\noperational')\n",
    "    ax_main.tick_params(axis='both', which='major', labelsize=6)\n",
    "    #plt.grid()\n",
    "    ax_main.grid()\n",
    "    fig.tight_layout()  \n",
    "\n",
    "    ax_side = plt.axes(rect_side)\n",
    "    binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "    #ax2 = fig.add_subplot(122)\n",
    "    #plt.hist(s,bins=binedges,color='black',orientation='horizontal')\n",
    "    ax_side.hist(s,bins=binedges,color='black',orientation='horizontal')\n",
    "    return (fig, ax_main, s)\n",
    "\n",
    "def numrunningperday_plot(data, yticks, yticklabels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    s = np.sum(data,axis=1)\n",
    "    plt.plot(s,'.',color='black',markersize=0.1)\n",
    "    plt.xticks(yticks, yticklabels)\n",
    "    ax.set_xticks(yticks, yticklabels)\n",
    "    ax.set_xticklabels(yticklabels, rotation = 90)\n",
    "    ax.set_ylabel('number\\noperational')\n",
    "    #ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax.grid()\n",
    "    fig.tight_layout()  \n",
    "    \n",
    "    unique, counts = np.unique(s, return_counts=True)\n",
    "    countsdf = pd.DataFrame(np.transpose([unique, counts]),columns=['numsites',\"days\"])\n",
    "    numdays = countsdf['days'].sum()\n",
    "    percent = countsdf['days']/numdays * 100\n",
    "    countsdf['frequency']=percent\n",
    "    print(countsdf)\n",
    "    print('Total number of days is %.0f' % numdays)\n",
    "    \n",
    "    #ax_side.hist(s,bins=binedges,color='black',orientation='horizontal')\n",
    "    return (fig, ax, s)\n",
    "\n",
    "def intday2date(intday):\n",
    "    ymd = str(intday)\n",
    "    d = dt.date(int(ymd[0:4]),int(ymd[4:6]),int(ymd[6:8]))\n",
    "    return d\n",
    "\n",
    "def fill_missing_days_in_df(df, startday=19950726, endday=20041231):\n",
    "    # need to make sure that every date from start to end exists in sites_df\n",
    "    cols = list(df.columns)\n",
    "    lastd = intday2date(startday)\n",
    "    totalmissingdays = 0\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            ymd = str(index)\n",
    "        except:\n",
    "            continue\n",
    "        d = intday2date(ymd)\n",
    "        daysdiff = (d-lastd).days\n",
    "        if daysdiff > 1:\n",
    "            totalmissingdays += daysdiff - 1\n",
    "            print(daysdiff,' days missing between', lastd, ' and ', d)\n",
    "            for d1 in range(1,daysdiff):\n",
    "                missingday = lastd + dt.timedelta(days=d1)\n",
    "                missingindex = int(missingday.strftime('%Y%m%d'))\n",
    "                for col in cols:\n",
    "                    df.at[missingindex,col]=0\n",
    "                #missingdict = {'date':), \\\n",
    "                #    'BE':0, 'CP':0, 'GA':0, 'GH':0, 'GT':0, 'HR':0, 'JH':0, 'LG':0, \\\n",
    "                #    'LY':0, 'MVO':0, 'NEV':0, 'PL':0, 'RY':0, 'SA':0, 'SP':0, 'SS':0, \\\n",
    "                #    'VP':0, 'WH':0}\n",
    "            #df = df.append(missingdict, ignore_index=True)\n",
    "        lastd = d\n",
    "    endd = intday2date(endday)   \n",
    "    if lastd < endd:\n",
    "        daysdiff = (endd-lastd).days\n",
    "        for d1 in range(1,daysdiff+1):\n",
    "            missingday = lastd + dt.timedelta(days=d1)\n",
    "            missingindex = int(missingday.strftime('%Y%m%d'))\n",
    "            for col in cols:\n",
    "                df.at[missingindex,col]=0\n",
    "    return(df.sort_index())\n",
    "\n",
    "def translate_id2site(id):\n",
    "    if len(id)==4:\n",
    "        id = 'MV.%s' % id\n",
    "    thissite = id[4:6]\n",
    "    if id[3:7]=='LONG':\n",
    "        thissite = 'LG'\n",
    "    if id[3:5]=='MW':\n",
    "        thissite = 'WH'\n",
    "    if id[3:7]=='MWH2':\n",
    "        thissite = 'WH2'       \n",
    "    if id[3:6]=='MVO':\n",
    "        thissite = 'MVO'\n",
    "    if id[4:7]=='NEV':\n",
    "        thissite = 'NEV'\n",
    "    if id[3:5]=='MP':\n",
    "        thissite = 'PL'\n",
    "    if id[3:6]=='MCH':\n",
    "        thissite = 'CP'\n",
    "    if id[3:6]=='MGT':\n",
    "        thissite = 'GA'\n",
    "    if id[3:7]=='IRIG' or id[3:7]=='TST1' or id[3:6]=='MRH':\n",
    "        thissite = ''\n",
    "    return thissite    \n",
    "\n",
    "def stationids2sites(uniqids):\n",
    "    sites = set()\n",
    "    for id in uniqids:\n",
    "        thissite = translate_id2site(id)\n",
    "        if not thissite:\n",
    "            continue\n",
    "        sites.add(thissite)\n",
    "    sites = sorted(sites)\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>WAV files per day per station</u></h1>\n",
    "\n",
    "Load in CSV file which has the number of events for each traceid each day<br/>\n",
    "Columns are yyyymmdd traceid count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 traceid  count\n",
      "yyyymmdd                       \n",
      "19950701  MV.MVPZ.--.SHZ      0\n",
      "19950701  MV.MWHE.--.SHE      0\n",
      "19950701  MV.MCPT.--.SHZ      0\n",
      "19950701  MV.MVOV.--.SHY      0\n",
      "19950701  MV.MGAT.--.SHZ      0\n",
      "                 traceid  count\n",
      "yyyymmdd                       \n",
      "20041231  MV.MWHN.--.SHX      0\n",
      "20041231  MV.MHRE.--.SHX      0\n",
      "20041231  MV.MPVE.--.SHZ      0\n",
      "20041231  MV.MWHT.--.SHZ      0\n",
      "20041231  MV.MPLX.--.SHE      0\n"
     ]
    }
   ],
   "source": [
    "dailytraceidfile = 'DATA/ASNE_dailytraceid_wavfiles_df.csv'\n",
    "trace_df = pd.read_csv(dailytraceidfile)\n",
    "trace_df = trace_df[['yyyymmdd','traceid','count']]\n",
    "trace_df = trace_df.set_index('yyyymmdd')\n",
    "print(trace_df.head())\n",
    "print(trace_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MV.MBET.--.SHZ', 'MV.MCHT.--.SHZ', 'MV.MCPE.--.SHX', 'MV.MCPN.--.SHY', 'MV.MCPN.--.SHZ', 'MV.MCPT.--.SHZ', 'MV.MCPZ.--.SHY', 'MV.MCPZ.--.SHZ', 'MV.MGAT.--.SHZ', 'MV.MGHZ.--.SHZ', 'MV.MGT2.--.SHZ', 'MV.MHRE.--.SHX', 'MV.MHRE.--.SHZ', 'MV.MHRN.--.SHX', 'MV.MHRN.--.SHY', 'MV.MHRV.--.SHY', 'MV.MHRV.--.SHZ', 'MV.MJHL.--.SHZ', 'MV.MJHT.--.SHZ', 'MV.MLGL.--.SHE', 'MV.MLGL.--.SHZ', 'MV.MLGT.--.SHZ', 'MV.MLYT.--.SHZ', 'MV.MNEV.--.SHZ', 'MV.MPEW.--.SHE', 'MV.MPEW.--.SHZ', 'MV.MPLX.--.SHE', 'MV.MPLY.--.SHN', 'MV.MPLZ.--.SHZ', 'MV.MPNS.--.SHN', 'MV.MPVE.--.SHZ', 'MV.MRHT.--.SHZ', 'MV.MRYT.--.SHZ', 'MV.MSAT.--.SHZ', 'MV.MSPT.--.SHZ', 'MV.MSSE.--.SHX', 'MV.MSSN.--.SHY', 'MV.MSSZ.--.SHZ', 'MV.MVOE.--.SHZ', 'MV.MVON.--.SHX', 'MV.MVOV.--.SHY', 'MV.MVPE.--.SHE', 'MV.MVPE.--.SHX', 'MV.MVPN.--.SHY', 'MV.MVPV.--.SHZ', 'MV.MVPZ.--.SHZ', 'MV.MWEH.--.SHX', 'MV.MWEL.--.SHX', 'MV.MWH2.--.SHZ', 'MV.MWHE.--.SHE', 'MV.MWHE.--.SHX', 'MV.MWHE.--.SHY', 'MV.MWHN.--.SHN', 'MV.MWHN.--.SHX', 'MV.MWHN.--.SHY', 'MV.MWHT.--.SHZ', 'MV.MWHZ.--.SHZ', 'MV.MWNH.--.SHY', 'MV.MWNL.--.SHY', 'MV.MWZH.--.SHZ', 'MV.MWZL.--.SHZ']\n"
     ]
    }
   ],
   "source": [
    "uniqids = sorted(trace_df.traceid.unique())\n",
    "station_ids = list(filter(lambda x: 'MV.M' in x, uniqids))\n",
    "print(station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_df = pd.DataFrame(columns = ['date'] + station_ids)\n",
    "for index, row in trace_df.iterrows():\n",
    "    #thisdate = row['yyyymmdd']\n",
    "    thisdate = index\n",
    "    thissta = row['traceid']\n",
    "    thiscount = row['count']\n",
    "    if np.isnan(thiscount):\n",
    "        thiscount = 0\n",
    "    station_df.at[thisdate,thissta] = thiscount\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-a46c9dcd915b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#station_df = station_df.drop(columns=['date'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstation_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstation_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_missing_days_in_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mfill_missing_days_in_df\u001b[0;34m(df, startday, endday)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# need to make sure that every date from start to end exists in sites_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mlastd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintday2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mtotalmissingdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mintday2date\u001b[0;34m(intday)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mintday2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mymd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Drop date column and fill missing days        \n",
    "#station_df = station_df.drop(columns=['date'])\n",
    "station_df = station_df.drop(['date'],axis=1)\n",
    "station_df = fill_missing_days_in_df(station_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>WAV file ontime per station</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-707e95c70ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstation_bool_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_bool_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert dataframe to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxticklabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstation_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_bool_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0montime_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstation_bool_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myticklabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'major'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mget_yticks\u001b[0;34m(thisdf)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthisdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0myticklabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Aren't these really independent NSLC's rather than just stations?\n",
    "station_bool_df = (station_df > 0) * 1\n",
    "station_bool_data = np.array(station_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = station_ids\n",
    "(yticks, yticklabels) = get_yticks(station_bool_df)\n",
    "(fig, ax) = ontime_plot(1-station_bool_data,xticklabels,yticks,yticklabels)\n",
    "ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/ASNE_station_ontime.pdf',dpi=300)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>number of stations per day in WAV files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    numsites    days  frequency\n",
      "0        0.0   550.0  15.841014\n",
      "1        7.0    48.0   1.382488\n",
      "2        8.0   413.0  11.895161\n",
      "3        9.0    86.0   2.476959\n",
      "4       10.0  1266.0  36.463134\n",
      "5       11.0   390.0  11.232719\n",
      "6       12.0    51.0   1.468894\n",
      "7       13.0   189.0   5.443548\n",
      "8       14.0   349.0  10.051843\n",
      "9       15.0   125.0   3.600230\n",
      "10      16.0     3.0   0.086406\n",
      "11      17.0     2.0   0.057604\n",
      "Total number of days is 3472\n"
     ]
    }
   ],
   "source": [
    "# So up to 17 channels. But mostly 10 (1266 days or 36.5%). 550 days with no data\n",
    "(yticks, yticklabels) = get_yticks(station_bool_df)\n",
    "(fig, ax, s) = numrunningperday_plot(station_bool_data, yticks, yticklabels)    \n",
    "plt.savefig('FIGURES/ASNE_numstationsperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Histogram of how many stations operational per day in WAV files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational NSLCs')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/ASNE_histnumstationsperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>WAV file analysis by SITE</u></h1>\n",
    "<h3>Convert WAV file data per station to per site</h3>\n",
    "loop over rows in df, and create a new dataframe for sites, with a 1 to indicate if on for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1536c9be4918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Drop date column and fill missing days\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msites_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msites_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0msites_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_missing_days_in_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msites_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: drop() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "uniqids = trace_df.traceid.unique()\n",
    "sites = stationids2sites(uniqids)\n",
    "sites_df = pd.DataFrame(columns = ['date'] + sorted(sites))\n",
    "for index, row in trace_df.iterrows():\n",
    "    #thisdate = row['yyyymmdd']\n",
    "    thisdate = index\n",
    "    thissite = translate_id2site(row['traceid'])\n",
    "    if not thissite:\n",
    "        continue\n",
    "    thiscount = row['count']\n",
    "    if np.isnan(thiscount):\n",
    "        thiscount = 0\n",
    "    currentcount = 0\n",
    "    try:\n",
    "        currentcount = sites_df.at[thisdate,thissite]\n",
    "    except:\n",
    "        pass\n",
    "    if np.isnan(currentcount):\n",
    "        currentcount = 0\n",
    "    if thiscount > currentcount:\n",
    "        sites_df.at[thisdate,thissite] = thiscount\n",
    "        \n",
    "# Drop date column and fill missing days        \n",
    "sites_df = sites_df.drop(columns=['date'])\n",
    "sites_df = fill_missing_days_in_df(sites_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>WAV file ontime per site</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-bc3285a17578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msites_bool_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msites_bool_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert dataframe to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxticklabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msites_bool_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0montime_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msites_bool_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myticklabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mget_yticks\u001b[0;34m(thisdf)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthisdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0myticklabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "sites_bool_df = (sites_df > 0) * 1\n",
    "sites_bool_data = np.array(sites_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = sites\n",
    "(yticks, yticklabels) = get_yticks(sites_bool_df)\n",
    "(fig, ax) = ontime_plot(1-sites_bool_data,xticklabels,yticks,yticklabels)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Site')\n",
    "fig.tight_layout()  \n",
    "plt.savefig('FIGURES/Figure7_ASNE_siteontime.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>number of sites per day in WAV files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   numsites    days  frequency\n",
      "0       5.0    50.0   1.711157\n",
      "1       6.0  1729.0  59.171800\n",
      "2       7.0   464.0  15.879535\n",
      "3       8.0   131.0   4.483231\n",
      "4       9.0   509.0  17.419576\n",
      "5      10.0    37.0   1.266256\n",
      "6      11.0     2.0   0.068446\n",
      "Total number of days is 2922\n"
     ]
    }
   ],
   "source": [
    "# 524 days with no data, that's 15.2% of the 3446 days.\n",
    "# 6 sites was most common: 50.2% of the time (1729 days)\n",
    "(fig, ax, s) = numrunningperday_plot(sites_bool_data, yticks, yticklabels)    \n",
    "plt.savefig('FIGURES/Figure8_ASNE_numsitesperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Histogram of how many sites operational per day in WAV files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/ASNE_sitesperday_histogram.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of WAV files per day</h3>\n",
    "This is based on the 2015 conversion, as is everything above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2922, 19)\n",
      "total # wavefiles = 230645\n"
     ]
    }
   ],
   "source": [
    "## This is a plot made from the 2015 ASNE_ database. It appears I had 230,645 files in that.\n",
    "sites_data = np.array(sites_df) # convert dataframe to numpy array\n",
    "print(sites_data.shape)\n",
    "y = np.nanmax(sites_data,axis=1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of WAV files per day', color=color)\n",
    "ax1.plot(y, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "plt.xticks(yticks, yticklabels)\n",
    "ax1.set_xticklabels(yticklabels, rotation = 90)\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Cumulative number of WAV files', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(np.cumsum(y), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#plt.show()\n",
    "plt.savefig('FIGURES/ASNE_numwavfilesperday.png',dpi=300)\n",
    "print('total # wavefiles = %d' % np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>WAV files per day from WAV files on Windows laptop</h3>\n",
    "This is the 2019 conversion.\n",
    "\n",
    "We re-create the previous figure from the countWAVfilesperday.py script run on the latest ASNE_ Miniseed file Seisan DB on Windows laptop. This was last modified on 2019/12/16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-025684c75559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#(yticks, yticklabels) = get_yticks(dailycounts_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_bool_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdailycounts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Existing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdailycounts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ShouldBe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mget_yticks\u001b[0;34m(thisdf)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthisdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'01'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0myticklabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "## This is a plot made from the 2019 ASNE_ database on the Windows laptop. This only goes to March 1st, 2001.\n",
    "## so we append data from the 2015 ASNE_ database from March 2, 2001, which corresponds to SEISLOG system.\n",
    "## We have only 230,112 files. Not sure why we are missing 533 files from the VDAP system period.\n",
    "dailycounts_df = pd.read_csv('DATA/countWAVfiles.dat',delimiter=' ')\n",
    "dailycounts_df['Date']\n",
    "dateformatter = lambda x: x.replace('-','')\n",
    "dailycounts_df['Date'] = dailycounts_df['Date'].apply(dateformatter)\n",
    "dailycounts_df = dailycounts_df.set_index('Date')\n",
    "\n",
    "plt.close('all')\n",
    "#(yticks, yticklabels) = get_yticks(dailycounts_df)\n",
    "(yticks, yticklabels) = get_yticks(station_bool_df)\n",
    "y = dailycounts_df['Existing']\n",
    "y2 = dailycounts_df['ShouldBe']\n",
    "yall = np.nanmax(sites_data,axis=1)\n",
    "yall[0:len(y)]=y\n",
    "fig, ax1 = plt.subplots()\n",
    "color1 = '0.0' # greyscale\n",
    "color2 = '0.3'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('waveform files per day', color=color1)\n",
    "ax1.plot(range(len(yall)), yall, linewidth=0.1, color=color1)\n",
    "ax1.plot(range(len(y2)), y2, 'o',  markersize=0.2,  color=color2)\n",
    "#ax1.plot(range(len(y)), y, linewidth=0.2, color=color1)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('Cumulative', color=color1)  # we already handled the x-label with ax1\n",
    "ax2.plot(np.cumsum(yall), '--', color=color1, linewidth=1.5)\n",
    "ax2.plot(np.cumsum(y), color=color2, linewidth=1.5)\n",
    "ax2.plot(np.cumsum(y2), color=color1, linestyle = ':', linewidth = 1)\n",
    "ax2.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "plt.xticks(yticks, yticklabels)\n",
    "ax1.set_xticklabels(yticklabels, rotation = 90)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('FIGURES/Figure5_ASNEnumWaveformFilesPerDay_grey.pdf',dpi=300)\n",
    "print('total # Miniseed files from 1995/07/27 to 2001/03/01 = %d' % np.sum(y))\n",
    "print('total # missing Miniseed files from 1995/07/27 to 2001/03/01 = %d' % (np.sum(y2) - np.sum(y)) )\n",
    "print('total # Seisan files from 2001/03/02 to 2004/12/16 = %d' % (np.sum(yall) - np.sum(y)))\n",
    "print('total # waveform files = %d' % np.sum(yall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the amount of RSAM data captured\n",
    "\n",
    "RSAM_data_captured.m was written on iMac in Documents/MATLAB to make a summary of what fraction of RSAM data was collected for each station and for each day of the ASN. This generated files like RSAM_MBET.txt consisting of two columns: date (yyyymmdd) \\t  MBET\n",
    "\n",
    "Next we read in all these files, and create a dataframe from merging all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MBET  MCPT  MCPZ  MGAT  MGHZ  MHRE  MHRN  MHRV  MJHL  MJHT  ...   \\\n",
      "date                                                                  ...    \n",
      "19950728  0.45   0.0   0.0  0.45  0.45   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "19950729  1.00   0.0   0.0  1.00  1.00   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "19950730  1.00   0.0   0.0  1.00  1.00   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "19950731  1.00   0.0   0.0  1.00  1.00   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "19950801  1.00   0.0   0.0  1.00  1.00   0.0   0.0   0.0   0.0   0.0  ...    \n",
      "\n",
      "          MWEL  MWH2  MWHE  MWHN  MWHT  MWHZ  MWNH  MWNL  MWZH  MWZL  \n",
      "date                                                                  \n",
      "19950728   0.0   0.0   0.0   0.0  0.45   0.0   0.0   0.0   0.0   0.0  \n",
      "19950729   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0   0.0   0.0  \n",
      "19950730   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0   0.0   0.0  \n",
      "19950731   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0   0.0   0.0  \n",
      "19950801   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "          MBET  MCPT  MCPZ  MGAT  MGHZ  MHRE  MHRN  MHRV  MJHL  MJHT  ...   \\\n",
      "date                                                                  ...    \n",
      "20040106   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0  1.00  1.00  ...    \n",
      "20040107   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0  1.00  1.00  ...    \n",
      "20040108   0.0   0.0   0.0   0.0  1.00   0.0   0.0   0.0  1.00  1.00  ...    \n",
      "20040109   0.0   0.0   0.0   0.0  0.97   0.0   0.0   0.0  0.97  0.97  ...    \n",
      "20040110   0.0   0.0   0.0   0.0  0.60   0.0   0.0   0.0  0.60  0.60  ...    \n",
      "\n",
      "          MWEL  MWH2  MWHE  MWHN  MWHT  MWHZ  MWNH  MWNL  MWZH  MWZL  \n",
      "date                                                                  \n",
      "20040106   0.0   0.0  1.00  1.00   0.0  1.00   0.0   0.0   0.0   0.0  \n",
      "20040107   0.0   0.0  1.00  1.00   0.0  1.00   0.0   0.0   0.0   0.0  \n",
      "20040108   0.0   0.0  1.00  1.00   0.0  1.00   0.0   0.0   0.0   0.0  \n",
      "20040109   0.0   0.0  0.97  0.97   0.0  0.97   0.0   0.0   0.0   0.0  \n",
      "20040110   0.0   0.0  0.60  0.60   0.0  0.60   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-1957e061659a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrsamdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrsamdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrsamdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_missing_days_in_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrsamdf_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m19950728\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20040110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mfill_missing_days_in_df\u001b[0;34m(df, startday, endday)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# need to make sure that every date from start to end exists in sites_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mlastd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintday2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mtotalmissingdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3fad3c8165c6>\u001b[0m in \u001b[0;36mintday2date\u001b[0;34m(intday)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mintday2date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mymd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "rsamtxtfiles = sorted(glob.glob('MATLAB/txtfiles/RSAM_????.txt'))\n",
    "count = 0\n",
    "for rsamfile in rsamtxtfiles:\n",
    "    rsamdf=pd.read_csv(rsamfile,sep='\\t')\n",
    "    if count == 0:\n",
    "        rsamdf_all = rsamdf\n",
    "    else:\n",
    "        rsamdf_all = pd.merge(rsamdf_all, rsamdf, on='date')\n",
    "    count += 1\n",
    "rsamdf_all = rsamdf_all.set_index('date')\n",
    "print(rsamdf_all.head())\n",
    "print(rsamdf_all.tail())\n",
    "rsamdf_all = fill_missing_days_in_df(rsamdf_all, startday=19950728, endday=20040110)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots summarizing how much RSAM data captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>First examine RSAM data by station</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rsam = np.array(rsamdf_all, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = rsamdf_all.columns\n",
    "(yticks, yticklabels) = get_yticks(rsamdf_all)\n",
    "(fig, ax) = ontime_plot(1-data_rsam,xticklabels,yticks,yticklabels)\n",
    "ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "plt.ylabel('Date')\n",
    "plt.xlabel('Station')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAMontime.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booldata_rsam = (data_rsam > 0) * 1\n",
    "(fig, ax, s) = numrunningperday_plot(booldata_rsam, yticks, yticklabels)    \n",
    "plt.ylabel('Number of RSAM stations operational')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAM_numstationsperday.pdf',dpi=300)\n",
    "#print(rsamdf_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(s,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/RSAM_numstationsperday_histogram.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now examine RSAM data by site</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamstations = list(xticklabels)\n",
    "rsamsites = stationids2sites(rsamstations)\n",
    "print(rsamsites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over rows in df, and create a new dataframe for sites, with a 1 to indicate if on for each day\n",
    "rsamsites_df = pd.DataFrame(columns = ['date'] + sorted(rsamsites))\n",
    "for index, row in rsamdf_all.iterrows():\n",
    "    for sta in rsamstations:\n",
    "        thisdate = index #row['date']\n",
    "        thissite = translate_id2site(sta)\n",
    "        #print(sta,thissite)\n",
    "        \n",
    "        if not thissite:\n",
    "            continue\n",
    "        thiscount = row[sta]\n",
    "        if np.isnan(thiscount):\n",
    "            thiscount = 0\n",
    "        currentcount = 0\n",
    "        try:\n",
    "            currentcount = rsamsites_df.at[thisdate,thissite]\n",
    "        except:\n",
    "            pass\n",
    "        if np.isnan(currentcount):\n",
    "            currentcount = 0\n",
    "        if thiscount > currentcount:\n",
    "            rsamsites_df.at[thisdate,thissite] = thiscount\n",
    "rsamsites_df = fill_missing_days_in_df(rsamsites_df, startday=19950728, endday=20040110)\n",
    "print(rsamsites_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamsites_bool_df = (rsamsites_df > 0) * 1\n",
    "data_rsam_sites = np.array(rsamsites_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "xticklabels = rsamsites\n",
    "(yticks, yticklabels) = get_yticks(rsamsites_bool_df)\n",
    "(fig, ax) = ontime_plot(1-data_rsam_sites[:,1:],xticklabels,yticks,yticklabels)\n",
    "plt.ylabel('Date')\n",
    "plt.xlabel('Site')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAM_siteontime.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax, srsam) = numrunningperday_plot(data_rsam_sites, yticks, yticklabels)    \n",
    "plt.ylabel('Number of sites operational')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAM_numsitesperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(srsam,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/RSAM_numsitesperday_histogram.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Combined analysis of WAV files and RSAM 1-minute data</u></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we combine RSAM and WAV files continuous data into same time on?\n",
    "xticklabels = sites\n",
    "combined_df = sites_bool_df.add(rsamsites_bool_df*0.4, fill_value=0)\n",
    "combined_df = combined_df.drop(columns='date')\n",
    "#print(continuous_bool_df)\n",
    "(yticks, yticklabels) = get_yticks(combined_df)\n",
    "combined_data = np.array(combined_df, dtype=float)/1.4\n",
    "print(combined_data)\n",
    "\n",
    "(fig, ax) = ontime_plot(1-combined_data,xticklabels,yticks,yticklabels)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Site')\n",
    "fig.tight_layout()  \n",
    "plt.savefig('FIGURES/ASNE_siteontime_continuous.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we combine sites per day from events and RSAM?\n",
    "combined_bool_data = (combined_data > 0) * 1\n",
    "(fig, ax, scombined) = numrunningperday_plot(combined_bool_data, yticks, yticklabels)    \n",
    "plt.ylabel('Number of sites operational')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/numsitesperday_combined.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(scombined,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.savefig('FIGURES/combined_numsitesperday_histogram.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns out we cannot combine them based on stations, because RSAM stations only record station code, whereas WAV files record whole NSLC\n",
    "station_bool_df = (station_df > 0) * 1\n",
    "station_bool_data = np.array(station_bool_df, dtype=float) # convert dataframe to numpy array\n",
    "combined_station_df = station_bool_df.add(rsamdf_all*0.4, fill_value=0)\n",
    "combined_station_data = np.array(combined_station_df, dtype=float)/1.4\n",
    "(yticks, yticklabels) = get_yticks(combined_station_df)\n",
    "xticklabels = station_ids\n",
    "(fig, ax) = ontime_plot(1-combined_station_data,xticklabels,yticks,yticklabels)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Station')\n",
    "fig.tight_layout()  \n",
    "ax.tick_params(axis='both', which='major', labelsize=4)\n",
    "# These do not combine as can be seen below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TILT data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilttxtfiles = sorted(glob.glob('MATLAB/txtfiles/TILT_????.txt'))\n",
    "count = 0\n",
    "for tiltfile in tilttxtfiles:\n",
    "    tiltdf=pd.read_csv(tiltfile,sep='\\t')\n",
    "    if count == 0:\n",
    "        tiltdf_all = tiltdf\n",
    "    else:\n",
    "        tiltdf_all = pd.merge(tiltdf_all, tiltdf, on='date')\n",
    "    count += 1\n",
    "tiltdf_all = tiltdf_all.set_index('date')\n",
    "print(tiltdf_all.head())\n",
    "print(tiltdf_all.tail())\n",
    "tiltdf_all = fill_missing_days_in_df(tiltdf_all, startday=19950727, endday=20001006)\n",
    "tiltdf_all = tiltdf_all.drop(columns=['CENT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiltdf_all = tiltdf_all[tiltdf_all.index <= 20001006] \n",
    "data_tilt = np.array(tiltdf_all, dtype=float) # convert dataframe to numpy array\n",
    "\n",
    "xticklabels = tiltdf_all.columns\n",
    "(yticks, yticklabels) = get_yticks(tiltdf_all)\n",
    "(fig, ax) = ontime_plot(1-data_tilt,xticklabels,yticks,yticklabels)\n",
    "\n",
    "plt.ylabel('Date')\n",
    "plt.xlabel('Station')\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/TILT_ontime.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tilt_bool = np.round((data_tilt > 0)*1)\n",
    "(fig, ax, stilt) = numrunningperday_plot(data_tilt_bool, yticks, yticklabels)    \n",
    "plt.ylabel('Number of TILT sites operational')\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/TILT_numsitesperday.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = np.arange(int(np.max(s))+2)-0.5\n",
    "plt.hist(stilt,bins=binedges,color='black')\n",
    "plt.xlabel('Number of operational TILT sites')\n",
    "plt.ylabel('Number of days')\n",
    "plt.xticks([0, 1, 2, 3])\n",
    "plt.savefig('FIGURES/TILT_numsitesperday_histogram.pdf',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>Event counts, hypocenters and magnitudes</u></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read concatenated HYPO71 summary file into a DataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    otime        lat        lon  depth   mag\n",
      "0     2000-01-13 23:31:42  16.712167  62.175167   2.00      \n",
      "1     2000-01-17 12:52:27  16.712833  62.175667   1.93      \n",
      "2     2000-01-17 16:13:56  16.705000  62.174667   2.00      \n",
      "3     2000-01-17 23:42:43  16.720000  62.178333   2.00      \n",
      "4     2000-01-22 17:37:27  16.713500  62.175500   2.72   0.4\n",
      "5     2000-01-22 17:41:29  16.711167  62.176333   2.76   0.1\n",
      "6     2000-01-26 22:07:28  16.707333  62.175000   2.61      \n",
      "7     2000-02-04 13:19:15  16.715833  62.173167   3.84   0.3\n",
      "8     2000-02-09 03:28:09  16.711500  62.174667   1.98      \n",
      "9     2000-02-13 17:36:35  16.717833  62.172167   1.92      \n",
      "10    2000-02-24 22:34:23  16.708833  62.176500   1.89      \n",
      "11    2000-02-27 00:13:12  16.713000  62.175333   2.61   0.2\n",
      "12    2000-03-07 15:40:13  16.712667  62.175333   2.76   1.1\n",
      "13    2000-03-07 15:41:56  16.714500  62.175333   2.08      \n",
      "14    2000-03-07 15:41:56  16.714500  62.175500   2.04   0.2\n",
      "15    2000-03-07 15:42:22  16.714833  62.174333   2.70   0.3\n",
      "16    2000-03-07 15:43:51  16.715500  62.174667   2.63   0.3\n",
      "17    2000-03-07 15:43:51  16.715833  62.174833   2.62      \n",
      "18    2000-03-07 15:47:27  16.709833  62.180833   2.47      \n",
      "19    2000-03-07 15:50:11  16.710500  62.175000   2.00   0.3\n",
      "20    2000-03-07 15:52:23  16.713667  62.175000   2.74      \n",
      "21    2000-03-07 16:29:14  16.711167  62.175000   1.94      \n",
      "22    2000-03-07 16:37:24  16.712500  62.176000   1.92      \n",
      "23    2000-03-09 21:08:57  16.711833  62.176500   2.77      \n",
      "24    2000-03-09 22:07:59  16.713667  62.174333   2.73      \n",
      "25    2000-03-10 03:15:29  16.712167  62.175167   2.77      \n",
      "26    2000-03-23 00:04:40  16.713333  62.175167   2.15      \n",
      "27    2000-03-23 00:26:33  16.712167  62.175167   2.39      \n",
      "28    2000-03-28 15:45:48  16.711833  62.174667   2.74      \n",
      "29    2000-04-17 07:48:26  16.720833  62.173500   2.69      \n",
      "...                   ...        ...        ...    ...   ...\n",
      "27601 1999-11-06 22:39:22  16.711333  62.176000   1.98      \n",
      "27602 1999-11-06 23:25:12  16.710000  62.176333   1.94      \n",
      "27603 1999-11-06 04:13:12  16.712333  62.175500   2.18      \n",
      "27604 1999-11-06 04:08:31  16.710167  62.177000   2.00      \n",
      "27605 1999-11-07 14:40:16  16.711333  62.173667   2.57   1.0\n",
      "27606 1999-11-07 18:28:36  16.712000  62.172833   2.99      \n",
      "27607 1999-11-07 18:39:34  16.710167  62.175167   2.29      \n",
      "27608 1999-11-07 20:02:17  16.712500  62.174333   2.16   1.1\n",
      "27609 1999-11-07 07:13:47  16.707500  62.179333   2.65      \n",
      "27610 1999-11-07 07:07:40  16.709833  62.175833   2.78      \n",
      "27611 1999-11-08 11:39:01  16.708333  62.178167   2.35      \n",
      "27612 1999-11-08 11:46:17  16.709333  62.176833   2.32      \n",
      "27613 1999-11-25 19:10:13  16.713833  62.170833   3.43      \n",
      "27614 1999-11-29 08:45:33  16.713333  62.173667   2.25   1.0\n",
      "27615 1999-12-02 15:47:42  16.713833  62.175000   2.83  -0.1\n",
      "27616 1999-12-02 15:51:49  16.714167  62.174833   2.80   0.0\n",
      "27617 1999-12-02 15:52:17  16.713500  62.176333   2.19   0.3\n",
      "27618 1999-12-02 15:55:39  16.711500  62.175500   2.13   0.2\n",
      "27619 1999-12-02 15:55:59  16.712500  62.175167   2.75   0.2\n",
      "27620 1999-12-02 15:56:54  16.710333  62.175667   2.23   0.2\n",
      "27621 1999-12-02 15:57:56  16.715667  62.173833   3.32   0.3\n",
      "27622 1999-12-02 15:58:25  16.709667  62.175833   1.96   0.2\n",
      "27623 1999-12-02 15:58:42  16.713500  62.175667   2.12   0.4\n",
      "27624 1999-12-02 15:59:10  16.714667  62.175500   1.96   0.4\n",
      "27625 1999-12-02 16:00:03  16.714500  62.175667   2.25   0.4\n",
      "27626 1999-12-02 16:01:16  16.713000  62.175333   2.10   0.4\n",
      "27627 1999-12-02 16:01:57  16.712333  62.175333   2.74   0.3\n",
      "27628 1999-12-02 16:02:46  16.713500  62.175500   1.96   0.2\n",
      "27629 1999-12-02 16:03:35  16.709833  62.176000   1.98      \n",
      "27630 1999-12-07 22:26:37  16.729333  62.167333   2.28   1.1\n",
      "\n",
      "[27631 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "hypodf = pd.DataFrame(columns=['otime','lat','lon','depth','mag'])\n",
    "#summaryfile = '/Volumes/data/Montserrat/MASTERING/VDAP/Hypocentres/summaryall_plus_willy.txt'\n",
    "summaryfile = 'DATA/summaryall_plus_willy.txt'\n",
    "#summaryfile = '/Users/thompsong/Documents/MATLAB/gdrive/data/MVO/seismicdata/copied_to_newton/Hypocentres/summaryall.txt'\n",
    "with open(summaryfile, 'r') as f1:\n",
    "    for str in f1:\n",
    "        try:\n",
    "            yy = float(str[0:2].strip())\n",
    "            mm = float(str[2:4].strip())\n",
    "            dd = float(str[4:6].strip())\n",
    "            hr = float(str[7:9].strip())\n",
    "            mi = float(str[9:11].strip())\n",
    "            secstr = str[12:17].strip()\n",
    "            secstr=secstr.replace(':','.')\n",
    "            sec = float(secstr.strip())\n",
    "            lat = float(str[17:20].strip())\n",
    "            mlat = float(str[21:26].strip())\n",
    "            lon = float(str[27:30].strip())\n",
    "            mlon = float(str[31:36].strip())\n",
    "            depth = float(str[37:43].strip())\n",
    "            magstr = str[45:49].strip()\n",
    "        except:\n",
    "            print('Failed on:')\n",
    "            print(str)\n",
    "            continue\n",
    "        try:\n",
    "            magstr = \"%3.1f\" % float(magstr)\n",
    "        except:\n",
    "            magstr = \" \" * 3\n",
    "        nphase = float(str[50:53].strip())\n",
    "        rms = float(str[62:66].strip())\n",
    "        dlat = float(lat) + float(mlat)/60.0\n",
    "        dlon = float(lon) + float(mlon)/60.0\n",
    "        if yy < 80:\n",
    "            cen = 20\n",
    "        else:\n",
    "            cen = 19\n",
    "        if (sec<60.0):\n",
    "            otime = dt.datetime(int(cen*100+yy),int(mm),int(dd),int(hr),int(mi),int(sec))\n",
    "        else:\n",
    "            otime = dt.datetime(int(cen*100+yy),int(mm),int(dd),int(hr),int(mi),0) + dt.timedelta(seconds=int(sec))\n",
    "        newrow = {'otime':otime, 'lat':dlat, 'lon':dlon, 'depth':depth, 'mag':magstr}\n",
    "        #print(newrow)\n",
    "        hypodf = hypodf.append(newrow , ignore_index=True)\n",
    "print(hypodf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remove duplicate hypocenters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x125459be0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypodf = hypodf.sort_values(by='otime')\n",
    "x = hypodf.otime\n",
    "la=dt.datetime(1990,1,1,0,0,0)\n",
    "diffsecs = dt.timedelta(seconds=50)\n",
    "i=0\n",
    "secsdifflist = []\n",
    "indexes = []\n",
    "for a in x:\n",
    "    secsdiff = a - la\n",
    "    if secsdiff < diffsecs:\n",
    "        secsdifflist.append(secsdiff.seconds)\n",
    "    if secsdiff.seconds > 5:\n",
    "        indexes.append(i)\n",
    "    la=a\n",
    "    i+=1\n",
    "hypodf_duplicates_removed = hypodf.iloc[indexes]    \n",
    "secsdifflist = np.array(sorted(secsdifflist))\n",
    "values, counts = np.unique(secsdifflist, return_counts=True)\n",
    "plt.plot(values, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Save the hypocenter data with duplicates removed</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypodf_duplicates_removed.to_csv('hypocenters_duplicates_removed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See plot_asn_hypocenter_data.m in GitHub repo for plot of these data on Montserrat station map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot cumulative hypocenters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = hypodf_duplicates_removed.otime\n",
    "y = np.cumsum(np.ones((len(x), 1)))\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative hypocenters', color='black')\n",
    "ax1.plot(x, y, '.', color='black', label='event alarms')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/Hypocenters_timeseries.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-eruption seismicity plot\n",
    "\n",
    "On 2020/02/19 Joan Latchman provided daily earthquake counts from January 1992 to July 1995 as an Excel file. This is now saved as DATA/eqcounts19921995.csv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot event counts, hypocenters and magnitudes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATE  NO. OF EVENTS  CUMULATIVE COUNT REMARKS       date\n",
      "0     1992/01/01              0                 0       0 1992-01-01\n",
      "1     1992/01/02              0                 0       0 1992-01-02\n",
      "2     1992/01/03              0                 0       0 1992-01-03\n",
      "3     1992/01/04              0                 0       0 1992-01-04\n",
      "4     1992/01/05              0                 0       0 1992-01-05\n",
      "5     1992/01/06              0                 0       0 1992-01-06\n",
      "6     1992/01/07              0                 0       0 1992-01-07\n",
      "7     1992/01/08              0                 0       0 1992-01-08\n",
      "8     1992/01/09              0                 0       0 1992-01-09\n",
      "9     1992/01/10              0                 0       0 1992-01-10\n",
      "10    1992/01/11              0                 0       0 1992-01-11\n",
      "11    1992/01/12              0                 0       0 1992-01-12\n",
      "12    1992/01/13              0                 0       0 1992-01-13\n",
      "13    1992/01/14              0                 0       0 1992-01-14\n",
      "14    1992/01/15              0                 0       0 1992-01-15\n",
      "15    1992/01/16              0                 0       0 1992-01-16\n",
      "16    1992/01/17              0                 0       0 1992-01-17\n",
      "17    1992/01/18              0                 0       0 1992-01-18\n",
      "18    1992/01/19              0                 0       0 1992-01-19\n",
      "19    1992/01/20              0                 0       0 1992-01-20\n",
      "20    1992/01/21              0                 0       0 1992-01-21\n",
      "21    1992/01/22              0                 0       0 1992-01-22\n",
      "22    1992/01/23              0                 0       0 1992-01-23\n",
      "23    1992/01/24              1                 1       0 1992-01-24\n",
      "24    1992/01/25              0                 1       0 1992-01-25\n",
      "25    1992/01/26              0                 1       0 1992-01-26\n",
      "26    1992/01/27              0                 1       0 1992-01-27\n",
      "27    1992/01/28              0                 1       0 1992-01-28\n",
      "28    1992/01/29              0                 1       0 1992-01-29\n",
      "29    1992/01/30              0                 1       0 1992-01-30\n",
      "...          ...            ...               ...     ...        ...\n",
      "1278  1995/07/02              2              1499       0 1995-07-02\n",
      "1279  1995/07/03              1              1500       0 1995-07-03\n",
      "1280  1995/07/04              5              1505       0 1995-07-04\n",
      "1281  1995/07/05              3              1508       0 1995-07-05\n",
      "1282  1995/07/06              4              1512       0 1995-07-06\n",
      "1283  1995/07/07              2              1514       0 1995-07-07\n",
      "1284  1995/07/08              6              1520       0 1995-07-08\n",
      "1285  1995/07/09              5              1525       0 1995-07-09\n",
      "1286  1995/07/10              2              1527       0 1995-07-10\n",
      "1287  1995/07/11              1              1528       0 1995-07-11\n",
      "1288  1995/07/12              5              1533       0 1995-07-12\n",
      "1289  1995/07/13              0              1533       0 1995-07-13\n",
      "1290  1995/07/14             22              1555       0 1995-07-14\n",
      "1291  1995/07/15             33              1588       0 1995-07-15\n",
      "1292  1995/07/16             12              1600       0 1995-07-16\n",
      "1293  1995/07/17             15              1615       0 1995-07-17\n",
      "1294  1995/07/18              6              1621       0 1995-07-18\n",
      "1295  1995/07/19              9              1630       0 1995-07-19\n",
      "1296  1995/07/20             29              1659       0 1995-07-20\n",
      "1297  1995/07/21              4              1663       0 1995-07-21\n",
      "1298  1995/07/22             12              1675       0 1995-07-22\n",
      "1299  1995/07/23             11              1686       0 1995-07-23\n",
      "1300  1995/07/24             15              1701       0 1995-07-24\n",
      "1301  1995/07/25             17              1718       0 1995-07-25\n",
      "1302  1995/07/26             23              1741       0 1995-07-26\n",
      "1303  1995/07/27             25              1766       0 1995-07-27\n",
      "1304  1995/07/28             39              1805       0 1995-07-28\n",
      "1305  1995/07/29             11              1816       0 1995-07-29\n",
      "1306  1995/07/30             32              1848       0 1995-07-30\n",
      "1307  1995/07/31             19              1867       0 1995-07-31\n",
      "\n",
      "[1308 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "precountsfile = 'DATA/eqcounts19921995.csv'\n",
    "precountsdf = pd.read_csv(precountsfile)\n",
    "precountsdf['date']=pd.Series([pd.to_datetime(date) for date in precountsdf['DATE']])\n",
    "precountsdf = precountsdf.fillna(0)\n",
    "print(precountsdf)\n",
    "xpre = precountsdf.date\n",
    "ypre = precountsdf['CUMULATIVE COUNT'].apply(pd.to_numeric,errors='coerce')\n",
    "plt.close('all')\n",
    "fig92, ax92 = plt.subplots()\n",
    "ax92.set_xlabel('Date')\n",
    "ax92.set_ylabel('Cumulative number of events', color='black')\n",
    "ax92.plot(xpre, ypre, '-.', color='black', label='VT', linewidth=2)\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.show()\n",
    "plt.savefig('FIGURES/Figure2_preeventcounts.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9325\n",
      "total VTs: 36379\n",
      "total LPs: 10499\n",
      "total Hybrids: 45990\n",
      "total Rockfalls: 58828\n",
      "total Hypocenters: 11189\n",
      "total Magnitudes: 9325\n"
     ]
    }
   ],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(xpre, ypre, '-.', color='k', label='pre', linewidth=2)\n",
    "\n",
    "countsfile = 'MATLAB/spcounts_gt.csv'\n",
    "countsdf = pd.read_csv(countsfile)\n",
    "countsdf['date']=pd.Series([pd.to_datetime(date) for date in countsdf['date']])\n",
    "countsdf = countsdf.fillna(0)\n",
    "\n",
    "\n",
    "#x = countsdf.date\n",
    "#lta=''\n",
    "#i=0\n",
    "#for a in x:\n",
    "#    ta=type(a)\n",
    "#    if ta!=lta:\n",
    "#        print(ta)\n",
    "#        print(i)\n",
    "#    lta=ta\n",
    "#    i+=1\n",
    "#print(x)\n",
    "# row 525 is bad\n",
    "countsdf=countsdf.drop([525])\n",
    "#print(countsdf.iloc[525])\n",
    "x = countsdf.date\n",
    "\n",
    "y = countsdf['volcano-tectonic'].apply(pd.to_numeric,errors='coerce')\n",
    "cumvt = np.cumsum(y)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative number of events', color='black')\n",
    "ax1.plot(x, cumvt, color='black', label='VT', linewidth=2)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "y2 = countsdf['long-period'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.plot(x, np.cumsum(y2), color='0.5', label='LP', linewidth=3)\n",
    "\n",
    "y3 = countsdf['hybrid'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.plot(x, np.cumsum(y3), '--', color='black', label='Hybrid', linewidth=2)\n",
    "\n",
    "y4 = countsdf['rockfall'].apply(pd.to_numeric,errors='coerce')\n",
    "ax1.plot(x, np.cumsum(y4), ':', color='0.5', label='Rockfall', linewidth=2)\n",
    "\n",
    "x2 = hypodf_duplicates_removed.otime\n",
    "y5 = np.ones((len(x2), 1))\n",
    "#ax1.set_xlabel('Date')\n",
    "#ax1.set_ylabel('Cumulative hypocenters', color='black')\n",
    "ax1.plot(x2, np.cumsum(y5), color='0.25', label='hypocenters', linewidth=1)\n",
    "#plt.gcf().autofmt_xdate()\n",
    "\n",
    "# find hypocenters with magnitudes\n",
    "c = 0\n",
    "i = 0\n",
    "ind = []\n",
    "for mstr in hypodf_duplicates_removed.mag:\n",
    "    try:\n",
    "        m = float(mstr)\n",
    "        c+=1\n",
    "        ind.append(i)\n",
    "    except:\n",
    "        pass\n",
    "    i+=1\n",
    "print(c)\n",
    "magdf = hypodf_duplicates_removed.iloc[ind]\n",
    "x6 = magdf.otime\n",
    "y6 = np.ones((len(x6), 1))\n",
    "\n",
    "ax1.plot(x6, np.cumsum(y6), ':', color='0.25', label='magnitudes', linewidth=1)\n",
    "\n",
    "\n",
    "ax1.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "print('total VTs: %d' % np.sum(y))\n",
    "print('total LPs: %d' % np.sum(y2))\n",
    "print('total Hybrids: %d' % np.sum(y3))\n",
    "print('total Rockfalls: %d' % np.sum(y4))\n",
    "print('total Hypocenters: %d' % np.sum(y5))\n",
    "print('total Magnitudes: %d' % np.sum(y6))\n",
    "plt.savefig('FIGURES/Figure6_eventcounts.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1701\n"
     ]
    }
   ],
   "source": [
    "print(precountsdf.iloc[1300]['CUMULATIVE COUNT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1995-07-20\n",
      "1      1995-07-21\n",
      "2      1995-07-22\n",
      "3      1995-07-23\n",
      "4      1995-07-24\n",
      "5      1995-07-25\n",
      "6      1995-07-26\n",
      "7      1995-07-27\n",
      "8      1995-07-28\n",
      "9      1995-07-29\n",
      "10     1995-07-30\n",
      "11     1995-07-31\n",
      "12     1995-08-01\n",
      "13     1995-08-02\n",
      "14     1995-08-03\n",
      "15     1995-08-04\n",
      "16     1995-08-05\n",
      "17     1995-08-06\n",
      "18     1995-08-07\n",
      "19     1995-08-08\n",
      "20     1995-08-09\n",
      "21     1995-08-10\n",
      "22     1995-08-11\n",
      "23     1995-08-12\n",
      "24     1995-08-13\n",
      "25     1995-08-14\n",
      "26     1995-08-15\n",
      "27     1995-08-16\n",
      "28     1995-08-17\n",
      "29     1995-08-18\n",
      "          ...    \n",
      "1935   2000-11-14\n",
      "1936   2000-11-15\n",
      "1937   2000-11-16\n",
      "1938   2000-11-17\n",
      "1939   2000-11-18\n",
      "1940   2000-11-19\n",
      "1941   2000-11-20\n",
      "1942   2000-11-21\n",
      "1943   2000-11-22\n",
      "1944   2000-11-23\n",
      "1945   2000-11-24\n",
      "1946   2000-11-25\n",
      "1947   2000-11-26\n",
      "1948   2000-11-27\n",
      "1949   2000-11-28\n",
      "1950   2000-11-29\n",
      "1951   2000-11-30\n",
      "1952   2000-12-01\n",
      "1953   2000-12-02\n",
      "1954   2000-12-03\n",
      "1955   2000-12-04\n",
      "1956   2000-12-05\n",
      "1957   2000-12-06\n",
      "1958   2000-12-07\n",
      "1959   2000-12-08\n",
      "1960   2000-12-09\n",
      "1961   2000-12-10\n",
      "1962   2000-12-11\n",
      "1963   2000-12-12\n",
      "1964   2000-12-13\n",
      "Name: date, Length: 1964, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           2.0\n",
      "1           6.0\n",
      "2          13.0\n",
      "3          14.0\n",
      "4          27.0\n",
      "5          31.0\n",
      "6          51.0\n",
      "7          59.0\n",
      "8          66.0\n",
      "9         101.0\n",
      "10        110.0\n",
      "11        114.0\n",
      "12        117.0\n",
      "13        124.0\n",
      "14        133.0\n",
      "15        151.0\n",
      "16        203.0\n",
      "17        274.0\n",
      "18        293.0\n",
      "19        295.0\n",
      "20        305.0\n",
      "21        311.0\n",
      "22        317.0\n",
      "23        380.0\n",
      "24        444.0\n",
      "25        452.0\n",
      "26          NaN\n",
      "27        458.0\n",
      "28        476.0\n",
      "29        494.0\n",
      "         ...   \n",
      "1935    36370.0\n",
      "1936    36374.0\n",
      "1937    36374.0\n",
      "1938    36375.0\n",
      "1939    36375.0\n",
      "1940    36375.0\n",
      "1941    36375.0\n",
      "1942    36375.0\n",
      "1943    36376.0\n",
      "1944    36376.0\n",
      "1945    36376.0\n",
      "1946    36376.0\n",
      "1947    36376.0\n",
      "1948    36376.0\n",
      "1949    36376.0\n",
      "1950    36376.0\n",
      "1951    36376.0\n",
      "1952    36376.0\n",
      "1953    36377.0\n",
      "1954    36379.0\n",
      "1955    36379.0\n",
      "1956    36379.0\n",
      "1957    36379.0\n",
      "1958    36379.0\n",
      "1959    36379.0\n",
      "1960    36379.0\n",
      "1961    36379.0\n",
      "1962    36379.0\n",
      "1963    36379.0\n",
      "1964    36379.0\n",
      "Name: volcano-tectonic, Length: 1964, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(cumvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gutenberg-Richter b-value plot</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mags = []\n",
    "for mag in sorted(magdf.mag):\n",
    "    try:\n",
    "        fmag = float(mag)\n",
    "        mags.append(fmag)\n",
    "    except:\n",
    "        pass\n",
    "mags = np.array(mags)\n",
    "m_max = max(mags)\n",
    "m_min = min(mags)\n",
    "m = np.arange(m_min, m_max, 0.1)\n",
    "n = np.zeros([len(m), 1])\n",
    "i = 0\n",
    "for thism in m:\n",
    "    a = np.where(mags>thism)\n",
    "    n[i] = len(a[0]) \n",
    "    i += 1\n",
    "plt.semilogy(m, n, 'o')\n",
    "plt.xticks(m, rotation = 90)\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Number > Magnitude')\n",
    "plt.tight_layout()\n",
    "plt.savefig('FIGURES/bvalues.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>RSAM alarms</u></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsameventalarmsdf = pd.read_csv('DATA/rsam_events_alarms.csv')\n",
    "rsameventalarmsdf.DATE = pd.Series([pd.to_datetime(date) for date in rsameventalarmsdf.DATE])\n",
    "rsamtremoralarmsdf = pd.read_csv('DATA/rsam_tremor_alarms.csv')\n",
    "rsamtremoralarmsdf.DATE = pd.Series([pd.to_datetime(date) for date in rsamtremoralarmsdf.DATE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cumulative numbers of alarms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = rsameventalarmsdf.DATE\n",
    "y = np.cumsum(np.ones((len(x), 1)))\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative RSAM alarms', color='black')\n",
    "ax1.plot(x, y, '.', color='black', label='event alarms')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "x2 = rsamtremoralarmsdf.DATE\n",
    "y2 = np.cumsum(np.ones((len(x2), 1)))\n",
    "ax1.plot(x2, y2, '.', color='gray', label='tremor alarms')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "plt.grid()\n",
    "fig.tight_layout()\n",
    "plt.savefig('FIGURES/RSAMalarms.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cumulative RSAM amplitudes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = rsameventalarmsdf.DATE\n",
    "y = rsameventalarmsdf['MLYT.DATA']+rsameventalarmsdf['MJHT.DATA']+ \\\n",
    "rsameventalarmsdf['MRYT.DATA']+rsameventalarmsdf['MLGT.DATA']+ \\\n",
    "rsameventalarmsdf['MWHZ.DATA']+rsameventalarmsdf['MGHZ.DATA']\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative RSAM alarm amplitude', color='black')\n",
    "ax1.plot(x, np.cumsum(y), '.', color='black')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "x2 = rsamtremoralarmsdf.DATE\n",
    "y2 = rsamtremoralarmsdf['MLYT.DATA']+rsamtremoralarmsdf['MJHT.DATA']+ \\\n",
    "rsamtremoralarmsdf['MRYT.DATA']+rsamtremoralarmsdf['MLGT.DATA']+ \\\n",
    "rsamtremoralarmsdf['MWHZ.DATA']+rsamtremoralarmsdf['MGHZ.DATA']\n",
    "ax1.plot(x2, np.cumsum(y2), '.', color='gray')\n",
    "plt.savefig('FIGURES/RSAMalarms_amplitudes.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cumulative RSAM thresholds</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax1 = plt.subplots()\n",
    "x = rsameventalarmsdf.DATE\n",
    "# all stations\n",
    "y = rsameventalarmsdf['MLYT.THRESH']+rsameventalarmsdf['MJHT.THRESH']+ \\\n",
    "rsameventalarmsdf['MRYT.THRESH']+rsameventalarmsdf['MLGT.THRESH']+ \\\n",
    "rsameventalarmsdf['MWHZ.THRESH']+rsameventalarmsdf['MGHZ.THRESH']\n",
    "# one station\n",
    "y = rsameventalarmsdf['MRYT.THRESH'] \n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative RSAM alarm threshold', color='black')\n",
    "ax1.plot(x, y, color='black')\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# tremor alarms\n",
    "#x2 = rsamtremoralarmsdf.DATE\n",
    "#y2 = rsamtremoralarmsdf['MLYT.THRESH']+rsamtremoralarmsdf['MJHT.THRESH']+ \\\n",
    "#rsamtremoralarmsdf['MRYT.THRESH']+rsamtremoralarmsdf['MLGT.THRESH']+ \\\n",
    "#rsamtremoralarmsdf['MWHZ.THRESH']+rsamtremoralarmsdf['MGHZ.THRESH']\n",
    "#y = rsamtremoralarmsdf['MLGT.THRESH']\n",
    "#ax1.plot(x2, y2, color='gray')\n",
    "\n",
    "plt.savefig('FIGURES/RSAMalarms_thresholds.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><u>To do</u></h1>\n",
    "<ul>\n",
    "    <li>Number of classified events vs WAV files (possibly just July 1995 - October 1996)\n",
    "    </li>\n",
    "    </ul>\n",
    "<h3>RSAM EVENTS, TRIGGERS AND SSAM DATA</h3>\n",
    "<ul>\n",
    "<li>cumulative RSAM events (or triggers?) vs day\n",
    "<li>SSAM data\n",
    "<li>How to associate RSAM triggers?\n",
    "</ul>\n",
    "\n",
    "<h3>DSN data</h3>\n",
    "\n",
    "With the DSN data, we want to do similar things regarding DSNC_, BSAM and DSNE_ (MVOE_)\n",
    "\n",
    "<h3>Merged data</h3>\n",
    "<ul>\n",
    "    <li>Minutes per day of data at each site?\n",
    "    <li>got to merge the ASN and DSN catalogs properly\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11140/182388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "36379+45990+10499+58828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
